import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset

# æµ‹è¯•ç”¨çš„è„šæœ¬

# ====== è·¯å¾„é…ç½® ======
MODEL_DIR = "sft-model-n1/medium/final_model"  # è®­ç»ƒå®Œä¿å­˜çš„æ¨¡å‹ç›®å½•
VAL_DATA_PATH = "output/val_data_with_prompt.jsonl"  # å¯é€‰ï¼ŒéªŒè¯é›†æ•°æ®è·¯å¾„ï¼ˆå¦‚ä¸è¯„ä¼°å¯ä¸ºç©ºï¼‰
USE_EVAL = os.path.exists(VAL_DATA_PATH)

# ====== åŠ è½½æ¨¡å‹ä¸ tokenizer ======
print(f"ğŸ”„ åŠ è½½æ¨¡å‹ï¼š{MODEL_DIR}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)
model.eval()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {device}")

# ====== æ¨ç†å‡½æ•° ======
def generate(prompt, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            top_p=0.95,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id,
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ====== ç®€å•äº¤äº’æ¨¡å¼ ======
print("âœï¸ è¾“å…¥ä½ æƒ³æµ‹è¯•çš„ promptï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
while True:
    user_input = input("ğŸŸ¢ Prompt >>> ")
    if user_input.lower().strip() in ["q", "quit", "exit"]:
        break
    result = generate(user_input)
    print("ğŸ“˜ æ¨¡å‹ç”Ÿæˆï¼š", result)
    print("------")

# ====== è‡ªåŠ¨è¯„ä¼°éªŒè¯é›†ï¼ˆå¯é€‰ï¼‰ ======
if USE_EVAL:
    print("ğŸ” æ­£åœ¨åŠ è½½éªŒè¯é›†è¿›è¡Œè¯„ä¼°...")
    dataset = load_dataset("json", data_files={"validation": VAL_DATA_PATH}, split="validation")

    def preprocess(batch):
        full_texts = [p + c for p, c in zip(batch["prompt"], batch["completion"])]
        tokenized = tokenizer(
            full_texts,
            truncation=True,
            padding="max_length",
            max_length=512,
        )
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    tokenized_dataset = dataset.map(preprocess, batched=True)
    tokenized_dataset = tokenized_dataset.remove_columns(["prompt", "completion"])

    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        args=TrainingArguments(
            output_dir="./eval_tmp",
            per_device_eval_batch_size=2,
        ),
        eval_dataset=tokenized_dataset
    )

    metrics = trainer.evaluate()
    print("ğŸ“Š éªŒè¯é›†è¯„ä¼°ç»“æœï¼š", metrics)

print("âœ… æ¨ç† & è¯„ä¼°ç»“æŸã€‚")
