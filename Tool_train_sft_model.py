import os
import json
import threading
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, EarlyStoppingCallback
from datasets import load_dataset

# sftè®­ç»ƒè„šæœ¬

# ====== è¯»å– config.json é…ç½®å‚æ•° ======
with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

model_name = config["model_name"]
data_path = config["data_path"]
output_dir = config["output_dir"]

# ====== åŠ è½½ tokenizer å’Œæ¨¡å‹ ======
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # GPT2 é»˜è®¤æ—  pad token

model = AutoModelForCausalLM.from_pretrained(model_name)

# ====== åŠ è½½å¹¶é¢„å¤„ç†æ•°æ® ======
dataset = load_dataset("json", data_files={"train": data_path}, split="train")

def preprocess(batch):
    full_texts = [p + c for p, c in zip(batch["prompt"], batch["completion"])]
    
    tokenized = tokenizer(
        full_texts,
        truncation=True,
        padding="max_length",
        max_length=config["max_seq_length"],
    )
    
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized


tokenized_dataset = dataset.map(preprocess, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["prompt", "completion"])

def is_valid(sample):
    try:
        return all(
            isinstance(sample[k], list) and all(isinstance(i, int) for i in sample[k])
            for k in ["input_ids", "attention_mask", "labels"]
        )
    except:
        return False
tokenized_dataset = tokenized_dataset.filter(is_valid)

# ===== ä¸­é€”æ£€æµ‹ =====
print(f"æœ‰æ•ˆæ ·æœ¬æ•°é‡ï¼š{len(tokenized_dataset)}")
print("è¾“å‡ºæ¨¡å‹è·¯å¾„ä¸ºï¼š", output_dir)

# ====== è·å–æœ€è¿‘çš„ checkpointï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰ ======
def get_last_checkpoint(output_dir):
    if not os.path.exists(output_dir):
        return None
    checkpoints = [
        os.path.join(output_dir, d)
        for d in os.listdir(output_dir)
        if d.startswith("checkpoint-")
    ]
    if not checkpoints:
        return None
    return sorted(checkpoints, key=lambda x: int(x.split("-")[-1]))[-1]

last_checkpoint = get_last_checkpoint(output_dir)

# ====== è®­ç»ƒå‚æ•° ======
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=config["batch_size"],
    num_train_epochs=config["epochs"],
    learning_rate=config["learning_rate"],
    fp16=config["fp16"],
    logging_steps=20,
    save_strategy="epoch",
    save_total_limit=2,
    remove_unused_columns=False,
    auto_find_batch_size=True,
)

# ====== å¯åŠ¨è®­ç»ƒå™¨ ======
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# ====== è®¾ç½®è‡ªåŠ¨ä¸­æ­¢è®­ç»ƒæ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰ ======
MAX_TRAIN_TIME = 90 * 60  # 90åˆ†é’Ÿ

# è®­ç»ƒæ—¶é•¿ç»“æŸåä¸­æ–­è®­ç»ƒçš„å‡½æ•°
def interrupt_training():
    print(f"\nâ° è¾¾åˆ° {MAX_TRAIN_TIME // 60} åˆ†é’Ÿé™åˆ¶ï¼Œå°è¯•ä¸­æ–­è®­ç»ƒ...")
    raise TimeoutError("è®­ç»ƒæ—¶é—´å·²åˆ°ï¼Œè‡ªåŠ¨ä¸­æ–­ã€‚")

# å¯åŠ¨å®šæ—¶å™¨ï¼ˆåœ¨åå°çº¿ç¨‹æ‰§è¡Œä¸­æ–­ï¼‰
timer = threading.Timer(MAX_TRAIN_TIME, interrupt_training)
timer.start()

# ====== å¯åŠ¨è®­ç»ƒå™¨ ======
try:
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆä» checkpoint: {last_checkpoint}ï¼‰")
    trainer.train(resume_from_checkpoint=last_checkpoint)

except TimeoutError as e:
    print(f"ğŸ›‘ {str(e)}ï¼Œä¿å­˜ä¸­æ–­æ¨¡å‹...")
    trainer.save_model(os.path.join(output_dir, "interrupted_checkpoint"))

except Exception as e:
    print(f"âŒ è®­ç»ƒå‡ºé”™ï¼š{e}")
    trainer.save_model(os.path.join(output_dir, "interrupted_checkpoint"))

finally:
    timer.cancel()  # æ¸…é™¤å®šæ—¶å™¨
    print("âœ… è®­ç»ƒå®Œæˆâœ…")