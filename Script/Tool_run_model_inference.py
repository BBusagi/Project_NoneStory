from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ===== æ¨¡å‹è·¯å¾„ï¼ˆä½ ä¿å­˜å¥½çš„æ¨¡å‹ï¼‰ =====
MODEL_PATH = "sft-model-n1/medium/final_model"  # æ”¹æˆä½ çš„è·¯å¾„

# ===== åŠ è½½æ¨¡å‹å’Œ tokenizer =====
print(f"ğŸ“¦ åŠ è½½æ¨¡å‹ä¸­ï¼š{MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè¾“å…¥æ—¥è¯­ Prompt åæŒ‰å›è½¦ç”Ÿæˆï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")

# ===== äº¤äº’å¼è¾“å…¥å¾ªç¯ =====
while True:
    prompt = input("\nğŸŸ¢ Prompt >>> ").strip()
    if prompt.lower() in ["q", "quit", "exit"]:
        print("ğŸ‘‹ é€€å‡ºäº¤äº’æµ‹è¯•")
        break

    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=True,
            top_p=0.95,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id,
        )
    result = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    print("ğŸ“˜ ç”Ÿæˆç»“æœï¼š", result)
