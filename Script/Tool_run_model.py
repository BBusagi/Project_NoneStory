import os
import torch
import json
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

# è®¾ç½®æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ›¿æ¢ä¸ºä½ è®­ç»ƒä¿å­˜çš„ç›®å½•ï¼‰
with open("./config.json", "r", encoding="utf-8") as f:
    config = json.load(f)
base_dir = Path(__file__).parent.resolve().parent
MODEL_DIR = os.path.join(base_dir, "sft-model-n1", "medium", "checkpoint-2880")  # è®­ç»ƒå®Œä¿å­˜çš„æ¨¡å‹ç›®å½•
print(f"ğŸ”„ åŠ è½½æ¨¡å‹ï¼š{MODEL_DIR}")

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œtokenizer
model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)

# å¦‚æœæœ‰GPUï¼Œä½¿ç”¨GPUè¿›è¡Œæ¨ç†ï¼›å¦åˆ™ä½¿ç”¨CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# æµ‹è¯•è¾“å…¥æ–‡æœ¬ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è¿™ä¸ªæ–‡æœ¬ï¼‰
input_text = "1000å­—ç¨‹åº¦ã®æ‹æ„›å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"

# ä½¿ç”¨tokenizerå°†è¾“å…¥æ–‡æœ¬ç¼–ç æˆæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼
input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

# ç”Ÿæˆæ–‡æœ¬
output = model.generate(input_ids,
                        max_length=1024,   # æœ€å¤§ç”Ÿæˆé•¿åº¦
                        num_return_sequences=3,  # ç”Ÿæˆçš„æ–‡æœ¬æ•°é‡
                        do_sample=True,   # å¯ç”¨é‡‡æ ·ï¼ˆå¦åˆ™æ˜¯è´ªå¿ƒæœç´¢ï¼‰
                        top_p=0.92,       # nucleus sampling (p-å€¼)
                        temperature=1,   # æ¸©åº¦æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§
                        top_k=50,         # top-ké‡‡æ ·
                        repetition_penalty=1.2)  # é‡å¤æƒ©ç½š

# è§£ç ç”Ÿæˆçš„outputï¼Œè½¬å›ä¸ºå¯è¯»çš„æ–‡æœ¬
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
print("ç”Ÿæˆçš„æ–‡æœ¬ï¼š")
print(generated_text)
