import os
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset
from pathlib import Path

# ====== è¯»å– config.json é…ç½®å‚æ•° ======
with open("./config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

base_dir = Path(__file__).parent.resolve().parent
model_name = config["model_name"]
data_path = str((base_dir / config["data_path"]).resolve())
output_dir = str((base_dir / config["output_dir"]).resolve())

# ====== è·å–æœ€è¿‘çš„ checkpointï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰ ======
def get_last_checkpoint(output_dir):
    if not os.path.exists(output_dir):
        return None
    checkpoints = [
        os.path.join(output_dir, d)
        for d in os.listdir(output_dir)
        if d.startswith("checkpoint-")
    ]
    if not checkpoints:
        return None
    return sorted(checkpoints, key=lambda x: int(x.split("-")[-1]))[-1]

last_checkpoint = get_last_checkpoint(output_dir)

# ====== åŠ è½½ tokenizer å’Œæ¨¡å‹ ======
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # GPT2 é»˜è®¤æ—  pad token

model = None
try:
    model = AutoModelForCausalLM.from_pretrained(last_checkpoint if last_checkpoint else model_name)
    param_count = sum(p.numel() for p in model.parameters())
    model_dir_used = model.config._name_or_path
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå…± {param_count:,} ä¸ªå‚æ•°ï¼ŒåŠ è½½è‡ªï¼š{model_dir_used}")
    if len(model.state_dict()) == 0:
        print("âš ï¸ è­¦å‘Šï¼šmodel.state_dict() ä¸ºç©ºï¼Œæ¨¡å‹å¯èƒ½æœªåŠ è½½å®Œæ•´ï¼")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
    model = None

# ====== åŠ è½½å¹¶é¢„å¤„ç†æ•°æ® ======
dataset = load_dataset("json", data_files={"train": data_path}, split="train")

def preprocess(batch):
    full_texts = [p + c for p, c in zip(batch["prompt"], batch["completion"])]
    tokenized = tokenizer(
        full_texts,
        truncation=True,
        padding="max_length",
        max_length=config["max_seq_length"],
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(preprocess, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["prompt", "completion"])

def is_valid(sample):
    try:
        return all(
            isinstance(sample[k], list) and all(isinstance(i, int) for i in sample[k])
            for k in ["input_ids", "attention_mask", "labels"]
        )
    except:
        return False

tokenized_dataset = tokenized_dataset.filter(is_valid)

print(f"âœ… æœ‰æ•ˆæ ·æœ¬æ•°é‡ï¼š{len(tokenized_dataset)}")
print("ğŸ—‚ï¸ è¾“å‡ºæ¨¡å‹è·¯å¾„ä¸ºï¼š", output_dir)

if model is None or len(model.state_dict()) == 0:
    raise RuntimeError("âŒ æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")

# ====== è®­ç»ƒå‚æ•° ======
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=config["batch_size"],
    num_train_epochs=config["epochs"],
    learning_rate=config["learning_rate"],
    fp16=config["fp16"],
    logging_steps=20,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,
    remove_unused_columns=False,
    auto_find_batch_size=True,
)

# ====== å¯åŠ¨è®­ç»ƒå™¨ ======
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# ====== å¯åŠ¨è®­ç»ƒæµç¨‹ ======
try:
    if last_checkpoint is None:
        print("ğŸ†• å½“å‰ä¸ºæ–°æ¨¡å‹è®­ç»ƒ")
    else:
        print(f"ğŸ” æ£€æµ‹åˆ°å·²æœ‰ checkpointï¼š{last_checkpoint}ï¼Œå°†ç»§ç»­è®­ç»ƒ")
    trainer.train()

except Exception as e:
    print(f"âŒ è®­ç»ƒå‡ºé”™ï¼š{e}")
    if model is not None and len(model.state_dict()) > 0:
        interrupted_path = os.path.join(output_dir, "interrupted_checkpoint")
        trainer.save_model(interrupted_path)
        print(f"ğŸ’¾ å‡ºé”™æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{interrupted_path}")
    else:
        print("âš ï¸ æ¨¡å‹æ— æ•ˆï¼Œæœªä¿å­˜ä»»ä½•å†…å®¹")

finally:
    if model is not None and len(model.state_dict()) > 0:
        final_path = os.path.join(output_dir, "final_model")
        trainer.save_model(final_path)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ï¼š{final_path}")
    else:
        print("âš ï¸ æœªä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæ¨¡å‹æ— æ•ˆï¼‰")
    print("âœ… è®­ç»ƒå®Œæˆ âœ…")
