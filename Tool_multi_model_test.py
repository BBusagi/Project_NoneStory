import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

# ===== æ¨¡å‹åˆ—è¡¨ï¼ˆåç§° â†’ è·¯å¾„ï¼‰ =====
MODEL_PATHS = {
    "rinna-original": "rinna/japanese-gpt2-medium",
    "sft-v1": "sft-model-n1/medium/final_model",
    # ä½ å¯ä»¥ç»§ç»­æ·»åŠ æ¨¡å‹å¯¹æ¯”
}

# ===== å‚æ•°è®¾ç½® =====
max_new_tokens = 1024
temperature = 0.8
top_p = 0.95
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== åŠ è½½æ‰€æœ‰æ¨¡å‹å’Œ tokenizer =====
models = {}
print("ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
for name, path in MODEL_PATHS.items():
    tokenizer = AutoTokenizer.from_pretrained(path)
    model = AutoModelForCausalLM.from_pretrained(path).to(device).eval()
    models[name] = (tokenizer, model)
print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæ¯•\n")

# ===== ä¸»å¾ªç¯ï¼šè¾“å…¥ Prompt æµ‹è¯• =====
print("âœï¸ è¾“å…¥ promptï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
while True:
    prompt = input("ğŸŸ¢ Prompt >>> ").strip()
    if prompt.lower() in ["q", "quit", "exit"]:
        break

    print("\nğŸ” å¤šæ¨¡å‹ç”Ÿæˆå¯¹æ¯”ç»“æœï¼š")
    print("=" * 60)
    for name, (tokenizer, model) in models.items():
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=top_p,
                temperature=temperature,
                pad_token_id=tokenizer.eos_token_id,
            )
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nğŸ§  {name}ï¼š")
        print(result)
        print("-" * 60)
